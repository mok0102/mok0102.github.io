---
title: "[AI] PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer"
date: 2022-10-28T00:00:00+00:00
author: ì´ì •ëª©
layout: post
permalink: /physformer/
categories: AI
tags: [AI, rPPG, transformer, temporal difference convolution]
---

íšŒì‚¬ ì¸í„´ì„ í•˜ë©´ì„œ heart rate estimationì„ í•´ì•¼ í•˜ëŠ”ë° ì“¸ë§Œí•œ ê±¸ ëª»ì°¾ì•„ì„œ ì”ëœ© ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ê³  ìˆë‹¤ê°€ ì°¾ì€ ëª¨ë¸ì´ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ìœ¼ë¡œ í•´ì„œ í˜„ì¬ SOTA ëª¨ë¸ì´ë‹¤. 2D CNNê¸°ë°˜ì€ ê±°ì˜ ì‚¬ìš© ëª»í•  ê²°ê³¼ê°€ ë‚˜ì˜¤ê³ , ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì´ rPPGë§Œ predictí•´ì„œ ì‹¤ì œ rPPGë¡œ ì‹¬ë°•ìˆ˜ë¥¼ ê³„ì‚°í•´ë³´ë©´ ì²˜ì°¸í•œ ìˆ˜ì¤€ì˜ ê²°ê³¼ë§Œ ì–»ì„ ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° ì´ ëª¨ë¸ì€ rPPGì—ì„œ psdë¡œ ì‹¬ë°•ìˆ˜ë¡œ ì¶”ì¶œí•˜ì—¬ loss termì— í™œìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì— ì‹¬ë°•ìˆ˜ë§Œ í•„ìš”í•œ ë‚´ taskì—ì„œ ì‹¬ë°•ìˆ˜ê°€ ì“¸ë§Œí•œ ì •ë„ë¡œ ë‚˜ì˜¨ë‹¤. ì •ë§ í•œ ì¤„ê¸° ë¹›ì˜ ëª¨ë¸ì´ë‹¤.

[ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/2111.12082)

## ë°°ê²½ì§€ì‹ (ppgë€?)
[ë§í¬](https://news.samsungdisplay.com/30140)

- ì‹ ì²´ì—ì„œ ì–‡ì€ ë¶€ë¶„(ì†ë, ë³¼, ê·€)ì€ ë¹›ì´ ì•½ê°„ í†µê³¼ â†’ í˜ˆì•¡ì˜ íë¦„ ê´€ì°° ê°€ëŠ¥
- ì‹¬ì¥ì´ í”¼ë¥¼ ë³´ë‚´ê¸° ìœ„í•´ì„œ ë›¸ ë•Œ ìƒê¸°ëŠ” ë¯¸ì„¸í•œ ë³€í™” : ë§¥íŒŒ, Plethysmogram
    - ë§¥íŒŒì— ë”°ë¼ ë³€í•˜ëŠ” ë¯¸ì„¸í•œ í˜ˆë¥˜ëŸ‰ì„ ì¡°ì‚¬í•´ì„œ íŒŒì•….

PPGì„¼ì„œì—ì„œ í”¼ë¶€ë¡œ ë¹›ì„ ì  ë•Œ, í˜ˆë¥˜ëŸ‰ì— ë”°ë¼(ì‹¬ì¥ì´ ì´ì™„ ì‹œ í˜ˆë¥˜ëŸ‰ ì¦ê°€, ìˆ˜ì¶•ì‹œ í˜ˆë¥˜ëŸ‰ ê°ì†Œ â‡’ ì‹¬ì¥ì˜ ì´ì™„, ìˆ˜ì¶• ì£¼ê¸°ë¥¼ ì•Œ ìˆ˜ ìˆìŒ â‡’ ì‹¬ë°•ìˆ˜) í¡ìˆ˜ë˜ëŠ” ë¹›ì˜ ì–‘ì´ ë‹¬ë¼ì§€ë¯€ë¡œ ë¹›ì´ ì–¼ë§ˆë‚˜ í¡ìˆ˜ë˜ì—ˆëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ì—¬ í˜ˆì•¡ëŸ‰ì˜ ë³€í™”ë¥¼ detect

![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_1.png?raw=true)

- ì‹¬ë°•ìˆ˜ = $$1/PPI*60$$
- ë°˜ë©´ ecgëŠ” ì‹¬ë°•ë™ê³¼ ê´€ë ¨ë˜ì–´ ë‚˜íƒ€ë‚˜ëŠ” ì „ìœ„ë³€í™” (ì „ê¸°ì‹ í˜¸ ì„¸ê¸°ì™€ ê°„ê²©)

**rppgëŠ” remote PPGë¡œ, facial videoì—ì„œ ì–¼êµ´ì˜ í˜ˆë¥˜ ë³€í™”ëŸ‰ì„ ë³´ê³  ppgë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹**

# Introduction

Electrocardiography(ECG)ì™€ Photoplethysmography(PPG)ê°€ ì‹¬ì¥ í™œë™ì„ ì¸¡ì •í•˜ëŠ” ê°€ì¥ í° ë‘ ë°©ì‹ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ë‘ ë°©ì‹ ëª¨ë‘ ëª¸ì— ë¶€ì°©ë˜ì–´ì•¼ í•˜ëŠ” ë¶ˆí¸í•¨ì´ ìˆê¸° ë•Œë¬¸ì— remote Photoplethy-smography(rPPG) ê°€ ë– ì˜¤ë¥´ê³  ìˆë‹¤.

### 3 stage module
ë³´í†µì˜ rppg ì—°êµ¬ëŠ” 3 stageë¥¼ ëˆë‹¤. [paper](https://arxiv.org/pdf/2111.09748v1.pdf)

1. a pre-processing stage, to minimize nuisance variation (face ROI ì¶”ì¶œ ë° ì „ì²˜ë¦¬)
    - detection + skin segmentation and tracking/ landmark
    - ì–¼êµ´ ì´ë¯¸ì§€ ì¶”ì¶œì´ ì–¼ë§ˆë‚˜ ì˜ ë˜ëŠëƒê°€ rppgì˜ ì„±ëŠ¥ì„ ì¢Œì§€ìš°ì§€ í•´ì„œ ì´ˆë°˜ì—ëŠ” ì´ìª½ì´ ë§ì´ ë°œì „
2. ppg signal extraction stage
    - ex. CNN
3. a heart rate estimation stage from the estimated PPG signal (ppgë¥¼ bpm(heart rate)ë¡œ ë°”ê¾¸ëŠ” ë‹¨)
    - peak detection
    - FFT (PSD)


### ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ì‹

- analyze subtle color changes on facial regions of interest with classical signal processing approaches

ê·¸ ë‹¤ìŒ color ë¶€ë¶„ ì²˜ë¦¬ë¥¼ í†µí•œ feature ë§µ ë³€í™˜(color subspace transformation methods, which utilize all skin pixels for rPPG measurement)

### ìœ„ ë°©ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ROI preprocessëœ feature mapì—ì„œ rppgë¥¼ ì¶”ì¶œí•˜ëŠ” learning-based model ë“±ì¥ (non end-to-end)

- ROI based preprocessed signal representations(e.g., time-frequency map and spatio-temporal map) are generated first, and then learnable models could capture rPPG features from these maps.
- ê·¸ëŸ¬ë‚˜ êµ‰ì¥íˆ strictí•˜ê²Œ preprocessing ìš”êµ¬

### ì•„ì˜ˆ processingëœ feature mapì´ ì•„ë‹Œ video ìì²´ë¥¼ inputìœ¼ë¡œ ë°›ëŠ” end-to-end ëª¨ë¸ë“¤ì´ ë“±ì¥

- treat facial video frames as input and predict rPPG and other physiological signals directly
- ê·¸ëŸ¬ë‚˜
    - ì „ì²˜ë¦¬ë‹¨ì´ ì‚¬ë¼ì¡Œìœ¼ë¯€ë¡œ complex scenarios(e.g, head movement)ì—ì„œ ë¬´ë„ˆì ¸ë‚´ë¦¬ëŠ” ê²½í–¥ì´ ìˆìŒ
    - rPPG-unrelated featureê°€ í•™ìŠµì— ì£¼ ë¶€ë¶„ì„ ì°¨ì§€í•  ê°€ëŠ¥ì„± â†’ large performance decrease in realistic dataset
    - ì „ì²˜ë¦¬ë‹¨ì—ì„œ ì •ì œ+ì¦í­í•´ì¤¬ë˜ ë¶€ë¶„ì´ ì—†ìœ¼ë¯€ë¡œ ppgì¶”ì¶œ ë‹¨ì´ í›¨ì”¬ ì–´ë ¤ìš´ taskë¡œ ë°”ë€œ

ppg â†’ Heart rate

- ë³´í†µ ëª¨ë¸ outputì´ ppg ì¶”ì¶œì—ì„œ ëë‚˜ëŠ” ëª¨ë¸ì´ ë§ë‹¤
    
    â†’ heart rate ë³€í™˜ ì‹œ noise ì œê±° ë“± signal processingì´ ë˜ í•„ìš”í•  ìˆ˜ë„ ìˆìŒ
    
- dl basedì´ ì•„ë‹Œ traditional ë°©ë²•ë¡ ì„ ì‚¬ìš©
    - fft(PSD), peak detection
- ê·¸ë˜í”„ ê°œí˜•ì´ ë¹„ìŠ·í•œ ê²ƒ(mse)ê³¼ ì£¼ê¸°(bpm=heart rate)ê°€ ë¹„ìŠ·í•œ ê²ƒì€ ë‹¤ë¥´ë‹¤ê³  ìƒê°!
    - heart rateë¥¼ loss termì— ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ìƒ ppg ê·¸ë˜í”„ë³´ë‹¤ë„ heart rateê°€ ì‹¤ì œì™€ ë§ì´ ë‹¤ë¥¸ ê²½ìš°ê°€ ì¡´ì¬

### Transformerì˜ ì‚¬ìš©

- rppgì—ì„œë„ CNN based ëª¨ë¸ì´ ëŒ€ê±° ë“±ì¥
- NLP, image / video analysisì—ì„œë„ ì„±ê³µì ìœ¼ë¡œ ì‘ë™í–ˆìœ¼ë¯€ë¡œ rppgì—ì„œë„ ì ìš©í•  ìˆ˜ ìˆì„ ê²ƒ
    - ê¸°ì¡´ê³¼ ë‹¤ë¥¸ ì ì€, subtleí•œ í”½ì…€ ë³€í™”ì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì— global spatio-temporal perceptionì´ challenging
    - the long-range spatio-temporal attentionì´ í•„ìš”
- rPPG measurement from facial videos can be treated as a video sequence to signal sequence problem
    - LONG RANGE contextual clues should be exploited for semantic modeling

### Contribution

- powerful video temporal difference transformer backboneìœ¼ë¡œ ì´ë£¨ì–´ì§„ physformer ëª¨ë¸ì˜ ì œì•ˆ
    - long-range spatio-temporal relationshipì„ ê³ ë ¤í•œ ì²« rppg model
- PhysFormerì˜ supervise ë°©ë²•ë¡ ë“¤ ì œì•ˆ
    - label distribution learning
    - curriculum learning guided dynamic loss in frequency domain
- pretrain ì—†ì´ SOTA ë‹¬ì„±


# Related Works

## Remote physiological measurement

### Plenty of traditional hand-crafted approaches

Face inputì—ì„œ feature map ì¶”ì¶œ ë‹¨:
- Selective merging information from different color channels / different ROIs

feature mapì—ì„œ rppg ì¶”ì¶œ ë‹¨:
- to improve signal-to-noise ratio of the recovered rPPG signals, several signal decomposition methods such as independent component analysis(ICA), and matrix completionì´ ì œì•ˆ

### Learning based approachì˜ ë“±ì¥

- deep learning based approaches dominate the field of rPPG measurement due to the strong spatio-temporal representation capabilities.

- **not end-to-end (facial ROI ì „ì²˜ë¦¬ë‹¨ + rppg ì¶”ì¶œë‹¨)**
    - facial ROI based spatial-temporal signal map ìƒì„± ë‹¨:
        - alleviate the interference from non-skin regions
        - ROI ì „ì²˜ë¦¬ê°€ ê½¤ ë§ì€ ë¹„ì¤‘ì„ ì°¨ì§€
        - ROI ë°”ê¹¥ ë¶€ë¶„ì„ ì „í˜€ ê³ ë ¤í•˜ì§€ ëª»í•¨(ë°°ê²½ ë°ê¸° ë³€í™” ê°™ì€)
        
    - mapë“¤ì—ì„œ featureë¥¼ ì¡ì•„ë‚´ëŠ” rppgë‹¨ (2D-CNN)
        - ì‹¤ì‹œê°„ì„±ì—ëŠ” ì¢‹ìŒ
    - ë”°ë¼ì„œ spatial-temporal ì •ë³´ë“¤ì„ ê³ ë ¤í•˜ê¸°ê°€ í˜ë“¬

- **end-to-end**
    - ì¸ì ‘í•œ í”„ë ˆì„ë§Œ ê³ ë ¤í•˜ê³  long range relationshipì˜ ì£¼ê¸°ì ì¸ ì •ë³´ëŠ” ê³ ë ¤í•˜ì§€ ì•ŠìŒ
        
        (previous methods only consider the spatio-temporal rPPG features from adjacent frames and neglect the long-range relationship among quasi-periodic rPPG features)
        

## Transformer for vision tasks

> Most of these works are incompatible for long-video-sequence (over 150 frames)
> 

rppg in vision transformer

- trans-rppg
    - extracts rPPG features from the preprocessed signal maps via ViT for face 3D mask presentation attack detection
- efficientphys (temporal shift networks)
    - Based on the temporal shift networks, efficientphys adds several swin transformer layers for global spatial attention

ìœ„ ë‘ ëª¨ë¸ì€ non-end-to-endëª¨ë¸ (feature mapì—ì„œ rppg ì¶”ì¶œ ë‹¨)

# Materials and Methods

## PhysFormer Architecture.
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_2.png?raw=true)


### Shallow stem
- Videoë¥¼ ë°”ë¡œ patchë¡œ ìª¼ê°œì„œ multi head self attentionì— ë„£ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì–‡ì€ stemì„ í•œë²ˆ ê±°ì¹œ outputì„ patchë¡œ ìª¼ê°œì„œ MHSAì— ë„£ìŒ
    - coarseí•œ local spatio-temporal feature ì¶”ì¶œì— ë„ì›€
    - benefits the fast convergence and clearer subsequent global self-attention
- 3ê°œì˜ convolution blocksì´ë£¨ì–´ì ¸ ìˆë‹¤ (ì»¤ë„ ì‚¬ì´ì¦ˆ: 1x5x5, 3x3x3, 3x3x3)
- ê° convolutionì— Batch Normalization, ReLU, MaxPoolì„ ìˆœì°¨ì ìœ¼ë¡œ ì‚¬ìš©
- Pooling layerê°€ spational dimensionì„ ë°˜ìœ¼ë¡œ ë‚˜ëˆ„ë¯€ë¡œ
    - Facial video input sizeê°€ 3xTxHxW ì¼ë•Œ shallow stem output sizeëŠ” D x T x H/8 x W/8

### Tube tokenization

- ì´ shallow stemì˜ outputì´ Nê°œì˜ tube tokenìœ¼ë¡œ ë‚˜ë‰˜ì–´ì§€ê³ , ì´ë ‡ê²Œ ë‚˜ë‰˜ì–´ì§„ tube tokenë“¤ì´ Nê°œì˜ temporal transformer blockìœ¼ë¡œ ë“¤ì–´ê°€ê³ , global-local refined rPPG í”¼ì²˜ì¸ X_transì„ ì–»ì„ ìˆ˜ ìˆë‹¤. (X_tubeì™€ ë™ì¼í•œ dimension)
- stemì—ì„œ ë‚˜ì˜¨ outputì„ X_stemì´ë¼ê³  í• ë•Œ, X_stemì„ non-overlappingí•œ tube tokenë“¤ë¡œ ë‚˜ë‰˜ì–´ì§„ë‹¤.
- stemì´ ìˆìœ¼ë¯€ë¡œ position embeddingê³¼ì •ì„ ë”°ë¡œ ì§„í–‰í•´ì£¼ì§€ ì•Šì•˜ë‹¤.

### Temporal difference multi-head self-attention
- AutoHR ë…¼ë¬¸ì—ì„œ ë“±ì¥í•œ temporal difference convolution ê°œë…        
- ê¸°ì¡´ì˜ self attentionì€ Query, Key, Valueë¥¼ ë‹¨ìˆœíˆ matmul í•´ì£¼ëŠ” í˜•íƒœ(1x1 conv)
- matmul (point-wise linear projection) í•˜ëŠ” ëŒ€ì‹  Temporal difference convolutionì„ ì‚¬ìš©
    - ë™ì˜ìƒì˜ ë°ê¸°ì™€ ê°™ì€ ê°’ë“¤ì„ ì¸ì ‘í•œ í”„ë ˆì„ë¼ë¦¬ ë¹¼ì¤Œìœ¼ë¡œì¨ ë¬´ì‹œ ê°€ëŠ¥
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_3.png?raw=true)

- TDC with learnable w

![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_4.png?raw=true)

p0 : current spatio-temporal location, R : sampled local (3x3x3) neighborhood, Râ€™ : sampled adjacent neighborhood

- temporal difference convolutionì„ Q(query)/K(key)ì˜ projectionì— ì‚¬ìš©.
    
    â‡’ ë¯¸ë¬˜í•œ ìƒ‰ ë³€í™” ê°ì§€ë¥¼ ìœ„í•´ ë¯¸ì„¸ íŠ¹ì§•ì„ í¬ì°©í•  ìˆ˜ ìˆìŒ (can capture fine-grained local temporal difference features for subtle color change description)
    

![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_5.png?raw=true)

- V(value) projection ë•Œì—ëŠ” temporal difference convolution projectionì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ê¸°ì¡´ì˜ point-wise linear projectionì„ ì‚¬ìš©

ië²ˆì§¸ headì˜ self-attention:
![Untitled](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_6.png?raw=true)

- ê¸°ì¡´ì˜ self-attentionì‹ì²˜ëŸ¼ Ï„ =âˆšDh_ië¥¼ ì‚¬ìš©í•œ ê²ƒì´ rppgì—ì„œ íš¨ê³¼ê°€ ì¢‹ì§€ ëª»í–ˆìœ¼ë¯€ë¡œ, ë” ì‘ì€ Ï„ë¥¼ ì‚¬ìš©í•´ sharper attention activationì„ ì§„í–‰. (ë³€ìˆ˜ë¡œ ë°›ì•„ì„œ ì§„í–‰)
- ì—¬ê¸°ì— residual connection, layer normalizationì´ ë§ˆì§€ë§‰ìœ¼ë¡œ ì§„í–‰ëœë‹¤.

### Spatio-temporal feed-forward

- TD-MHSAì˜ outputì´ í”¼ë“œí¬ì›Œë“œ NNì˜ input
FFNN(x) = MAX(0, xW1+b1)*W2+b2 (ë‘ê°œì˜ linear ì¸µ)

- vanilla feed-forward network
    - ë‘ê°œì˜ linear transformation layerë¡œ êµ¬ì„±
    - ë‘ ë ˆì´ì–´ ì‚¬ì´ì—ì„œ ë” ë§ì€ feature í‘œí˜„ì„ ìœ„í•´ì„œ hidden dimensionì´ í™•ì¥ë¨ (the hidden dimension Dâ€™ between two layers is expanded to learn a richer feature representation)
    - temporalí•œ ì •ë³´ ê³ ë ¤ê°€ í˜ë“¬
    
- depthwise 3D convolution(with BN and nonlinear activation)ì„ ì´ ë‘ ë ˆì´ì–´ ì‚¬ì´ì— ì‚¬ìš©
    - ST-FFê°€ local inconsistencyì™€ parts of noisy featuresë¥¼ ë‹¤ë“¬ì–´ ì¤„ ìˆ˜ ìˆë‹¤ê³  ì œì•ˆ
    - richer localityëŠ” TD-MHSAì—ê²Œ ì¶©ë¶„í•œ relative position ì‹ í˜¸ë¥¼ ì œê³µ


## Label Distribution Learning

Facial age estimation taskì—ì„œ, ë‚˜ì´ ì°¨ì´ê°€ ì ì„ìˆ˜ë¡ ì–¼êµ´ì´ ë¹„ìŠ·í•´ë³´ì´ëŠ” ì ì„ ì´ìš©

â‡’ Facial rPPG signals with close HR values usually have similar periodicity.

- í•˜ë‚˜ì˜ ì–¼êµ´ videoì— ëŒ€í•œ outputì´ í•˜ë‚˜ì˜ label(HR)ì´ê¸°ë³´ë‹¤ëŠ”, label distributionì„ ì´ìš©í•´ outputì´ ì—¬ëŸ¬ ê°œì˜ HR value labelì— ëŒ€í•œ distributionìœ¼ë¡œ í‘œí˜„ë˜ë„ë¡ í•¨.
    - label distributionì€ HRì˜ íŠ¹ì • ë²”ìœ„(42, 180)ì„ í¬í•¨í•˜ê³ , outputì€ ê° labelë“¤ì´ ì–¼ë§ˆë‚˜ ìœ ë ¥í•œì§€ì— ëŒ€í•œ ì •ë„ë¥¼ í‘œí˜„í•¨.
    - Through this way, one facial video can contribute to both targeted HR value and its adjacent HRs.

ğŸ’¡ rPPG-based HR estimation problemì„ specific L-classes multi-label classification problemìœ¼ë¡œ ì •ì˜í–ˆë‹¤. (L=139, 42 to 180)

### GT labelì„ ì–´ë–»ê²Œ label distributionìœ¼ë¡œ ë§Œë“¤ì—ˆëŠ”ê°€?

- ê° labelì˜ entry pëŠ” [0,1] ì‚¬ì´ì˜ real valueì´ê³ , ëª¨ë“  labelì˜ p í•©ì€ 1ì´ ë˜ë„ë¡ í–ˆë‹¤. (í™•ë¥ )
- Gaussian distribution functionì„ ì´ìš©
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_7.png?raw=true)
- ì˜¤íˆë ¤ real distributionì„ ì‚¬ìš©í–ˆì„ ë•Œë³´ë‹¤ gaussian distributionì„ ì‚¬ìš©í–ˆì„ ë•Œ ë” ì„±ëŠ¥ì´ ê´œì°®ì•˜ë‹¤!

- ê²°êµ­ loss functionì€ L_LD = KL(p, Softmax(Ë†p)) ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. KLì€ Kullback-Leibler divergenceì´ê³ , p hatì€ power spectral density(PSD) of predicted rPPG signals. â‡’ rppgì—ì„œ HR ì¶”ì¶œì„ ì–´ë–»ê²Œ í–ˆëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ê³  ìˆìŒ
    - Kullback-Leibler divergence - ë‘ í™•ë¥ ë¶„í¬ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ (ì •ë³´ ì—”íŠ¸ë¡œí”¼ ì°¨ì´ë¥¼ ê³„ì‚°)
    - Power Spectral density(PSD)
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_8.png?raw=true)

ì´ì „ì—ë„ label distributionì„ ì´ìš©í•œ ì‚¬ë¡€(A robust rppg approach with distribution learning)ê°€ ìˆì—ˆì§€ë§Œ, ìš°ë¦¬ì™€ ì°¨ì´ì ì€

1. ì´ì „ì— ì´ ë°©ì‹ì„ ì‚¬ìš©í•œ ì´ìœ ëŠ” frameë§ˆë‹¤ì˜ ì–¼êµ´ ì›€ì§ì„ì— ëŒ€í•´ temporal HR outliersë¥¼ smoothení•˜ëŠ” ê²ƒì´ì§€ë§Œ, ìš°ë¦¬ëŠ” adjacent labelê°„ì˜ efficient featureë¥¼ ë°°ìš°ë„ë¡ í•˜ê¸° ìœ„í•´ ì§„í–‰í•˜ì˜€ë‹¤.
2. ì´ì „ ì—°êµ¬ì—ì„œëŠ” hand-crafted rppg signal ì¶”ì¶œ ì´í›„ì— HR-estimationë¥¼ ìœ„í•œ post HR estimationì´ì§€ë§Œ, ìš°ë¦¬ëŠ” ë”¥ëŸ¬ë‹ ë‹¨ì—ì„œ ì ìš©í•˜ì˜€ë‹¤.

## Curriculum Learning Guided Dynamic Loss

- Curriculum Learning - ì‰¬ìš´ taskë¶€í„° í•™ìŠµ

### temporal domain loss
- mean square error loss, negative Pearson loss
    - negative Pearson correlation : maximize the trend similarity and minimize peak location errors (Remote Photoplethysmograph Signal Measurement from Facial Videos Using Spatio-Temporal Networksì—ì„œ ë“±ì¥í•œ loss)
- signal-trend-level constraintsì„ ì£¼ë¯€ë¡œ ê¸ˆë°© í•™ìŠµ â†’ ì‰¬ìš´ overfitting

### frequency domain loss
- cross-entropy loss, signal to noise ratio loss
- target frequency bandsì—ì„œ ì£¼ê¸°ì ì¸ í”¼ì²˜ë“¤ì„ í•™ìŠµí•´ì•¼ í•¨
- realisticí•œ rPPG-irrelevant ë…¸ì´ì¦ˆ ë•Œë¬¸ì— ì£¼íŒŒìˆ˜ë‹¨ì˜ ê°•í•œ constraints

ì´ê²Œ ì¤‘ìš”í•˜ë‹¤. frequency domain lossê°€ ì‹¬ë°•ìˆ˜ë¥¼ loss termìœ¼ë¡œ í™œìš©í•  ê±°ë¼ëŠ” ë§ì´ê¸° ë•Œë¬¸ì— rPPG predictionì—ì„œë§Œ ì˜ ì‘ë™í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ heart rate estimationì—ì„œë„ ì˜ ì‘ë™í•  ìˆ˜ ìˆê²Œ ëœë‹¤.
 

ë˜í•œ, **dynamic supervision to gradually enlarge the frequency constraints**ë¥¼ ì œì•ˆ.
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_9.png?raw=true)
linear increment strategy/exponential increment strategy ì¤‘ exponential strategyë¥¼ ì ìš© (alpha = 0.1, beta_0 = 1.0, n = 5.0)

- pretrainì„ ì‚¬ìš©í•˜ë©´ HRë¡œë§Œìœ¼ë¡œë„ í•™ìŠµ ê°€ëŠ¥í•  ìˆ˜ë„ ìˆì„ ê²ƒ ê°™ë‹¤!! ì§€ê¸ˆ ê°€ì§€ê³  ìˆëŠ” rPPG ë°ì´í„°ê°€ ì—†ì–´ì„œ í•œ ì¤„ê¸° ë¹›ì´ ëœë‹¤.

# Results

ì„¸ê°œì˜ physiological signalì— ëŒ€í•´ ì§„í–‰ : 

heart rate(HR), heart rate variability(HRV), respiration frequency(RF)

### ì‚¬ìš©í•œ ë°ì´í„°ì…‹

### VIPL-HR

- large-scale dataset for remote physiological measurement under less-constrained scenarios
    - head movement, illuminationì„ í¬í•¨
- 2378 RGB videos of 107 subjects recorded with different head movements, lighting conditions and acquisition devices

### MAHNOB-HCI

- one of the most widely used benchmark for remote HR measurement evaluations
- 527 facial videos with 61 fps from 27 subjects

### MMSE-HR

- 102 RGB videos from 40 subjects

### OBF

- high-quality dataset for remote physiological signal emeasurement
- 200 5 min ong RGB videos with 60 fps recorded from 100 healthy adults

---

- mean HR estimationì€ ìœ„ì˜ ë„¤ê°œ ë°ì´í„°ì…‹ì—ì„œ ì§„í–‰
- HRV, RF estimationì€ OBFì—ì„œë§Œ ì§„í–‰
    - follow existing methods, and report low frequency(LF), high frequency(HF), and LF/HF ratio for HRV and RF estimation
- SD, MAE, RMSE, pcc(peasonâ€™s correlation coefficient)

### Implementation Details

- MTCNN face detectorì„ ì´ìš©í•˜ì—¬ ì²«ë²ˆì§¸ í”„ë ˆì„ì˜ enlarged face areaë¥¼ crop, ì´í›„ frameë“¤ë„ ì´ regionìœ¼ë¡œ fix
- MAHNOB-HCI, OBFëŠ” fps=30ìœ¼ë¡œ ë‚´ë ¸ìŒ
- Physformer ì„¸íŒ… :
    - N=12, h=4, D=96, Dâ€™=144,
    - TD-MHSA : Î¸=0.7 (for TDC), Ï„=2.0
    - tube size T_s x H_s x W_s = 4x4x4
- trainì‹œì— randomly sample RGB face clips with size 160x128x128(TxHxW)
- Random horizontal flipping and temporally up/down-sampling for data augmentation
- Adam optimizer
- lr = 1e-4, weight decay = 5e-5
- batch size=4
- 25 epochìœ¼ë¡œ
    - alpha=0.1 for temporal loss,
    - exponentially increased parameter beta=[1,5] for frequency loss.
- Ïƒ=1.0 for label distriution learning (gaussian distribution)
- testing : seperate 30-second videos into three short clips with 10 seconds, and video-level HR is calculated via averaging HR from three short clips

### Intra-dataset Testing

### HR estimation on VIPL-HR

- traditional methods(Tulyakov2016, POS, CHROM) perform poorly due to the complex scenarios
- end-to-end learning based methods(PhysNet, DeepPhys, AutoHR) predict unreliable HR values with large RMSE compared with non-end-to-end learning approaches(PhythmNet, ST-Attention, NAS-HR, CVD, Dual-GAN)
- all five non-end-to-end methods first extract fine-grained signal maps from multiple facial ROIs, and then more dedicated rPPG clues would be extracted via the cascaded models.
- PhysformerëŠ” pretrain ì—†ì´ë„ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¸ë‹¤!

![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_10.png?raw=true)

### HR estimation on MAHNOB-HCI
- finetune the VIPL-HR pretrained model on MAHNOB-HCI for further 15 epoc
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_11.png?raw=true)

### HR, HRV and RF estimation on OBF
Physformer also gives more accurate estimation in terms of HR, RF, and LF/HF compared with the preprocessed signal map based non-end-to-end learning method CVD.
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_12.png?raw=true)

### Cross-dataset Testing

- VIPLì— train í›„ MMSE-HRì— finetune í•˜ì§€ ì•Šê³  ë°”ë¡œ testing
- SOTA ë‹¬ì„±
1. The predicted HRs are highly correlated with the ground truth HRs
2. The model learns domain invariant intrinsic rPPG-aware features.

## Ablation Study

**Impact of tube tokenization**
four tokenization configuration ìœ ë¬´
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_13.png?raw=true)
- stemì„ ì‚¬ìš©í–ˆì„ ë•Œê°€ ë” ê²°ê³¼ê°€ ì¢‹ì•˜ìŒ
- input resolutionì„ ë‚´ë¦´ìˆ˜ë¡ ê²°ê³¼ê°€ ì•ˆì¢‹ì•„ì§

**Impact of TD-MHSA and ST-FF**
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_14.png?raw=true)

- Spatio-temporal attention ì—†ì„ ë•Œ performance degrades sharply
- Ï„ê°€ MHSAì— ì˜í–¥ì„ ë§ì´ ì¤€ë‹¤ â‡’ ViTì²˜ëŸ¼ íƒ€ìš°ë¥¼ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ drop
- ViTë³´ë‹¤ Ï„ë¥¼ ì‘ê²Œ ì¡ì•˜ì„ ë•Œ(sharp) ì„±ëŠ¥ì´ ë” í–¥ìƒ ë˜ì—ˆë‹¤.

**Impact of label distribution learning**
**Impact of dynamic supervision**
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_15.png?raw=true)
- Temporal lossì™€ frequency cross-entropy lossì—ì„œ, frequency lossìª½ì´ label distribution
- label distributionì„ exponential strategyë¡œ ì‚¬ìš©í–ˆì„ ë•Œ ê°€ì¥ ê²°ê³¼ê°€ ì¢‹ìŒ
- label distributionì„ ì•„ì˜ˆ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ì„ ë•ŒëŠ”, exponential strategyë¥¼ ì ìš©í•´ë„ ê²°ê³¼ê°€ ì¢‹ì§€ ì•Šì•˜ìŒ
    - It is interesting to find from the last two rows that using real PSD distribution from ground truth PPG signals as p, the performance is inferior due to the lack of an obvious peak and partial noise. (real PSD distributionë³´ë‹¤ ê°€ìš°ì‹œì•ˆ distributionì´ ë” ì¢‹ì•˜ë‹¤)

- Fold-1 VIPL-HRë¡œ fixed/dynamic supervisionì„ ì‹¤í—˜í•˜ì˜€ì„ ë•Œ
![ì´ë¯¸ì§€](https://github.com/JungMok-Lee/JungMok-Lee.github.io/blob/master/assets/images/2022_10_28_16.png?raw=true)

**Impact of theta and layer/head numbers**

Physformer could achieve smaller RMSE when theta=0.4 and 0.7, indicating the importance of the normalized local temporal difference features for global spatio-temporal attention.

With deeper temporal transformer blocks, the RMSE are reduced progressively despite heavier computational cost.

In terms of the impact of head numbers, it is clear to find that PhysFormer with four heads perform the best while fewer heads lead to sharp performance drops.

# Conclusion

- rppgë¥¼ ìœ„í•œ End-to-end video transformer architectureë¥¼ ë§Œë“¬
- Temporal difference transformerë¥¼ ì‚¬ìš©í•œ ì„±ëŠ¥ í–¥ìƒ

- ë” íš¨ìœ¨ì ì¸ architecture
    - ëª¨ë°”ì¼ì—ì„œëŠ” ì‚¬ìš©í•˜ê¸° ì–´ë ¤ìš´ parameter ìˆ˜
- More accurate yet efficient spatio-temporal self-attention mechanism for long sequence rPPG monitoring

rPPGë¼ëŠ” ê²Œ ì–¼êµ´ì—ì„œ ì‚¬ëŒì˜ ëˆˆìœ¼ë¡œ ê°ì§€í•˜ê¸° ì–´ë ¤ìš´ í˜ˆìƒ‰ì˜ ë³€í™”ë¥¼ ê°ì§€í•´ì„œ ppg ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì•¼ í•˜ëŠ” ë¬¸ì œì´ê¸° ë•Œë¬¸ì—, ì´ê²Œ ì§„ì§œ ê°€ëŠ¥í•œê±°ëƒë¼ê³  ë§í•˜ëŠ” ì‚¬ëŒë“¤ë„ ìˆê³  ë‚˜ë„ ì–´ëŠ ì •ë„ëŠ” ë™ê°í•œë‹¤. ê·¸ë˜ì„œ ë§ì€ rPPG ì¶”ì¶œ ëª¨ë¸ë“¤ì´ ì´ subtleí•œ í˜ˆìƒ‰ ë³€í™”ë¥¼ ì¦í­ì‹œí‚¬ ìˆ˜ ìˆëŠ” ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ”ë° ì—¬ê¸°ì„œëŠ” ê·¸ê²Œ TDCì˜€ë˜ ê±°ë‹¤. 

ê·¸ëŸ°ë° ì§ì ‘ physformer ì‹¤í—˜ì„ í•´ë³´ë©´ 30fpsì—ì„œëŠ” ì˜ ì‘ë™í•˜ë”ë¼ë„ frame interpolationìœ¼ë¡œ fpsë¥¼ ë°”ê¿”ì¤¬ì„ ë•Œ ì˜ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ê²Œ ëª¨ë¸ì´ 30 fpsì—ì„œë§Œ ì£¼ë¡œ í•™ìŠµì´ ë˜ì–´ì„œ ì¼ìˆ˜ë„ ìˆì§€ë§Œ, ë‚´ê°€ ìƒê°í•˜ê¸°ì—ëŠ” ëª¨ë¸ì´ í”„ë ˆì„ ê°„ì˜ ì°¨ì´(temporal difference)ë¥¼ ë³´ê³  ìˆë‹¤ê¸°ë³´ë‹¤ ê·¸ ìˆœê°„ì˜ í˜ˆìƒ‰ì´ ì–¼ë§ˆë‚˜ ë¶‰ì€ì§€ë¥¼ ë³´ëŠ”, temporal ì •ë³´ë¥¼ ê³ ë ¤í•˜ê³  ìˆì§€ ì•Šì„ ìˆ˜ë„ ìˆëŠ” ê²ƒ ê°™ë‹¤. ì‚¬ëŒ í˜ˆìƒ‰ì´ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ì£¼ê¸°ì ìœ¼ë¡œ ë³€í•˜ëƒë³´ë‹¤ë„, íŠ¹ì • í”„ë ˆì„ì—ì„œ ì‚¬ëŒ ì–¼êµ´ì´ ë¶‰ìœ¼ë©´ ì´ ì‚¬ëŒì´ ìš´ë™ ì¤‘ì´ë¼ê³  ìƒê°í•˜ê³  ë†’ì€ ì‹¬ë°•ìˆ˜ë¥¼ ë±‰ëŠ” ê²½í–¥ì„±ì´ ìˆë‹¤. ê·¸ë˜ì„œ ì¸ì¢…ì— ëŒ€í•œ biasë„ ë§ì´ ê±¸ë¦¬ê³ , ì•„ë¬´íŠ¼ ì°¸ ì–´ë ¤ìš´ ë¬¸ì œì´ë‹¤.